#! /usr/bin/env bash
# GACODE Parallel execution script (PERLMUTTER_GPU)

export MPICH_GPU_SUPPORT_ENABLED=1

#env 1>&2

#echo $SLURM_LOCALID
let r4=SLURM_LOCALID/4
let l4="$SLURM_LOCALID-($r4*4)"
# GPU3 is connected to CPU0
let ACC_DEVICE_NUM=3-${l4}
export ACC_DEVICE_NUM
export OMP_DEFAULT_DEVICE=${ACC_DEVICE_NUM}

echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID $ACC_DEVICE_NUM `taskset -pc $$`"
#ecno "uname -n` $SLURM_LOCALID LL $LD_LIBRARY_PATH"

NTASKS_PER_NODE=$((SLURM_NTASKS / SLURM_NNODES))

if [ ${NTASKS_PER_NODE} -gt 4 ]; then
  # use MPS
  # https://docs.nvidia.com/deploy/mps/index.html
  export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
  export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log

  NODE_RANK=$((SLURM_PROCID % NTASKS_PER_NODE))

  if [ $NODE_RANK -eq 0 ]; then
    echo $SLURM_PROCID starting nvidia-cuda-mps-control on $(hostname)
    nvidia-cuda-mps-control -d
  fi

  sleep 5

  if [ $NODE_RANK -eq 0 ]; then
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID mps ready: $(date --iso-8601=seconds)"
  fi

  "$@"

  if [ $NODE_RANK -eq 0 ]; then
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopping nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
    echo quit | nvidia-cuda-mps-control
    echo "`uname -n` $SLURM_PROCID $SLURM_LOCALID stopped nvidia-cuda-mps-control: $(date --iso-8601=seconds)"
  fi
else
  # no need for MPS
  exec  "$@"
fi

